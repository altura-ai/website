---
title: "Quickprop, an Alternative to Back-Propagation"
date: 2020-08-25T14:51:12+06:00
author: Johanna Appel
image_webp: images/jellyfish_rose_small.jpg
image: images/jellyfish_rose_small.jpg
description : "Scott Fahlman’s idea to speed up gradient descent"
---

Due to the slowly converging nature of the vanilla back-propagation algorithms of the ’80s/’90s, Scott Fahlman invented a learning algorithm dubbed Quickprop that is roughly based on Newton’s method. His simple idea outperformed back-propagation (with various adjustments) on problem domains like the ‘N-M-N Encoder’ task — i.e. training a de-/encoder network with N inputs, M hidden units and N outputs.

[Continue reading on Medium](https://towardsdatascience.com/quickprop-an-alternative-to-back-propagation-d9a78069e2a7)
